{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: \n",
    "PM2.5 :Airborne particulate matter (PM) is not a single pollutant, but rather is a mixture of many chemical species. It is a complex mixture of solids and aerosols composed of small droplets of liquid, dry solid fragments, and solid cores with liquid coatings. Particles vary widely in size, shape and chemical composition, and may contain inorganic ions, metallic compounds, elemental carbon, organic compounds, and compounds from the earth’s crust. Particles are defined by their diameter for air quality regulatory purposes. Those with a diameter of 10 microns or less (PM10) are inhalable into the lungs and can induce adverse health effects. Fine particulate matter is defined as particles that are 2.5 microns or less in diameter (PM2.5). Therefore, PM2.5 comprises a portion of PM10.\n",
    "\n",
    "- Variation in size, shape and chemical composition\n",
    "- contain inorganic ions, metallic compunds usw.\n",
    "- Particles are defined by their diameter for air quality regulatory purposes\n",
    "- diameter of 10 microns or less PM10 - are inhalable\n",
    "- PM 2.5 microns or less in diameters -- portion of pm10\n",
    "\n",
    "\n",
    "1. What is PM2.5\n",
    "2. How does time matter and how do we split it?\n",
    "3. What columns play a role to predict the target? \n",
    "4. NaN, Nulls, Data Cleaning, afterwards\n",
    "5. can we add extra date from the internet?\n",
    "\n",
    "\n",
    "The World Health Organization sets acceptable PM2.5 levels at 10 µg/m³ annually and at 25 µg/m³ in the 24-hour window.\n",
    "\n",
    "its called forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To understand the meaning of the columns in your dataset, you can take several approaches. Here are a few options:\n",
    "\n",
    "1. Dataset Documentation or Data Dictionary\n",
    "If your dataset is part of a larger project or publicly available, it may come with accompanying documentation or a data dictionary. This would explain each column and its significance. Look for any README files, documentation, or metadata that came with the dataset.\n",
    "\n",
    "2. Column Naming Conventions\n",
    "Sometimes column names can give you a clue about their meaning based on standard naming conventions. For example:\n",
    "\n",
    "target: This could represent the dependent variable, likely air pollution concentration (e.g., PM2.5, NO2, etc.).\n",
    "precipitable_water_entire_atmosphere: This might refer to the total water content in a vertical column of the atmosphere.\n",
    "relative_humidity_2m_above_ground: Likely the relative humidity measured at 2 meters above the ground.\n",
    "L3_NO2_NO2_column_number_density: Refers to the density of NO2 in a vertical atmospheric column.\n",
    "These names often reflect the physical or environmental phenomena being measured, though understanding them fully might require some domain-specific knowledge (e.g., meteorology, atmospheric science).\n",
    "\n",
    "3. Consult with the Data Provider or Use External Resources\n",
    "If you obtained this data from an organization (e.g., NASA, a governmental agency, or a scientific repository), there is often accompanying documentation on their website or in related publications that describe the columns.\n",
    "\n",
    "For scientific datasets, you might need to reference papers or studies that provide detailed explanations about how the data is collected and what the specific metrics mean.\n",
    "\n",
    "4. Investigate Common Terms\n",
    "If the dataset includes environmental, meteorological, or pollutant-specific columns, here are a few general references you can look up:\n",
    "\n",
    "NO2, CO, SO2, O3, CH4: These are common air pollutants (Nitrogen Dioxide, Carbon Monoxide, Sulfur Dioxide, Ozone, Methane, etc.), and columns like L3_NO2_NO2_column_number_density might indicate concentrations of these substances.\n",
    "Humidity and Wind Components: Terms like \"humidity\" and \"wind component\" are related to atmospheric conditions, with columns likely measuring these at specific heights or in specific orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by different location and create differend panda frames for it, and for each of them we create a sub_df and target, and we do .shift(1) what it will do it will shift by one, it will give us yesterdays value if we shift by one\n",
    "\n",
    "for that we can create new variable, sub df leg 1 \n",
    "\n",
    "we create a new column, feature engineering, where for each place, what is the observation of the day before, but before this we need to do the train test split, it should also be in a forward looking fashion, take the last three days for each location as test set, in a forecasting problem we dont do long term prediction, its okay to take the alst 3 days as test set. Its gonna be location based. \n",
    "\n",
    "What are the two? create yesterday observation as a feature with groupby magic\n",
    "generic thing create test set, forward looking test, last three days, it needs also group by manipulation\n",
    "\n",
    "\n",
    "MAPE mean absolute error \n",
    "\n",
    "\n",
    "train set has \n",
    "day 1 - day 60\n",
    "\n",
    "and test set\n",
    "\n",
    "day 61, 62, 63\n",
    "\n",
    "we need to add column which location, some are target Place_ID  - NEW column lag 1 --> day before (once created we do XGBOOST or linear regression on it - time aspect), first we create lage 1\n",
    "\n",
    "sub df for each location as part of the est set\n",
    "\n",
    "pearson correlation can be computed even with missing values, to see which features are not so powerful - for feature selection! \n",
    "\n",
    "measure performance, other complexity\n",
    "\n",
    "how do we feed sub dfs in regression?\n",
    "\n",
    "Baseline model lag1 feature - what is the target of the day before - drop the dates where no date before is (like first date) - lag 1 is corresponding to its own place id, so than the date problem is gone and we can feed it in the regression\n",
    "\n",
    "linear regression (Xtrain, Ytrain - polluted yesterday?\n",
    "\n",
    "lag 1 on our test data is the prediction but not the ground root! - to build approximation on approximation is - time series problem extremely noisy - tmrw becomes new data, continuesoly predict on for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Custom Transformer for Group-Wise Imputation\n",
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_col, strategy='mean'):\n",
    "        self.group_col = group_col\n",
    "        self.strategy = strategy\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Separate the group column from the features\n",
    "        self.columns_ = X.columns.drop(self.group_col)\n",
    "        \n",
    "        # Compute group-wise statistics (mean) for each numerical feature\n",
    "        self.impute_values_ = X.groupby(self.group_col).transform(lambda x: x.mean())\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Fill missing values based on group statistics\n",
    "        X_filled = X.copy()\n",
    "        for col in self.columns_:\n",
    "            # Use the group-wise mean for each numerical feature\n",
    "            X_filled[col] = X_filled[col].fillna(self.impute_values_[col])\n",
    "        return X_filled.drop(columns=[self.group_col])\n",
    "\n",
    "# Step 2: Define numerical and categorical features\n",
    "numerical_features = [\"L3_CO_CO_column_number_density\", \n",
    "                      \"L3_HCHO_tropospheric_HCHO_column_number_density\",\n",
    "                      \"L3_NO2_NO2_column_number_density\", \n",
    "                      \"L3_O3_O3_column_number_density\", \n",
    "                      \"u_component_of_wind_10m_above_ground\"]\n",
    "\n",
    "categorical_features = ['Place_ID']  # Ensure 'Place_ID' exists in your DataFrame\n",
    "\n",
    "# Step 3: Preprocess the data (Group-Wise Imputation + Fallback Imputation + Scaling/Encoding)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('group_imputer', GroupImputer(group_col='Place_ID')),  # Custom group-wise imputation\n",
    "            ('fallback_imputer', SimpleImputer(strategy='mean')),  # Fallback imputer for any remaining NaNs\n",
    "            ('scaler', StandardScaler())  # Standardize the numerical features\n",
    "        ]), numerical_features + ['Place_ID']),  # Pass Place_ID for group imputation\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)  # OneHotEncode Place_ID\n",
    "    ])\n",
    "\n",
    "# Define the target variable\n",
    "y_train = df_train_split[\"target\"]\n",
    "X_train = df_train_split[numerical_features + categorical_features]\n",
    "\n",
    "y_test = df_test_split[\"target\"]\n",
    "X_test = df_test_split[numerical_features + categorical_features]\n",
    "\n",
    "# Step 6: Create a pipeline with preprocessing and linear regression\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Step 7: Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Make predictions and evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate the performance\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape :.2f}%')\n",
    "print(f'R² Score: {r2}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
