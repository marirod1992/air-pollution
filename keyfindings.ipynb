{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time from timedata import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target: \n",
    "PM2.5 :Airborne particulate matter (PM) is not a single pollutant, but rather is a mixture of many chemical species. It is a complex mixture of solids and aerosols composed of small droplets of liquid, dry solid fragments, and solid cores with liquid coatings. Particles vary widely in size, shape and chemical composition, and may contain inorganic ions, metallic compounds, elemental carbon, organic compounds, and compounds from the earth’s crust. Particles are defined by their diameter for air quality regulatory purposes. Those with a diameter of 10 microns or less (PM10) are inhalable into the lungs and can induce adverse health effects. Fine particulate matter is defined as particles that are 2.5 microns or less in diameter (PM2.5). Therefore, PM2.5 comprises a portion of PM10.\n",
    "\n",
    "- Variation in size, shape and chemical composition\n",
    "- contain inorganic ions, metallic compunds usw.\n",
    "- Particles are defined by their diameter for air quality regulatory purposes\n",
    "- diameter of 10 microns or less PM10 - are inhalable\n",
    "- PM 2.5 microns or less in diameters -- portion of pm10\n",
    "\n",
    "\n",
    "1. What is PM2.5\n",
    "2. How does time matter and how do we split it?\n",
    "3. What columns play a role to predict the target? \n",
    "4. NaN, Nulls, Data Cleaning, afterwards\n",
    "5. can we add extra date from the internet?\n",
    "\n",
    "\n",
    "The World Health Organization sets acceptable PM2.5 levels at 10 µg/m³ annually and at 25 µg/m³ in the 24-hour window.\n",
    "\n",
    "its called forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To understand the meaning of the columns in your dataset, you can take several approaches. Here are a few options:\n",
    "\n",
    "1. Dataset Documentation or Data Dictionary\n",
    "If your dataset is part of a larger project or publicly available, it may come with accompanying documentation or a data dictionary. This would explain each column and its significance. Look for any README files, documentation, or metadata that came with the dataset.\n",
    "\n",
    "2. Column Naming Conventions\n",
    "Sometimes column names can give you a clue about their meaning based on standard naming conventions. For example:\n",
    "\n",
    "target: This could represent the dependent variable, likely air pollution concentration (e.g., PM2.5, NO2, etc.).\n",
    "precipitable_water_entire_atmosphere: This might refer to the total water content in a vertical column of the atmosphere.\n",
    "relative_humidity_2m_above_ground: Likely the relative humidity measured at 2 meters above the ground.\n",
    "L3_NO2_NO2_column_number_density: Refers to the density of NO2 in a vertical atmospheric column.\n",
    "These names often reflect the physical or environmental phenomena being measured, though understanding them fully might require some domain-specific knowledge (e.g., meteorology, atmospheric science).\n",
    "\n",
    "3. Consult with the Data Provider or Use External Resources\n",
    "If you obtained this data from an organization (e.g., NASA, a governmental agency, or a scientific repository), there is often accompanying documentation on their website or in related publications that describe the columns.\n",
    "\n",
    "For scientific datasets, you might need to reference papers or studies that provide detailed explanations about how the data is collected and what the specific metrics mean.\n",
    "\n",
    "4. Investigate Common Terms\n",
    "If the dataset includes environmental, meteorological, or pollutant-specific columns, here are a few general references you can look up:\n",
    "\n",
    "NO2, CO, SO2, O3, CH4: These are common air pollutants (Nitrogen Dioxide, Carbon Monoxide, Sulfur Dioxide, Ozone, Methane, etc.), and columns like L3_NO2_NO2_column_number_density might indicate concentrations of these substances.\n",
    "Humidity and Wind Components: Terms like \"humidity\" and \"wind component\" are related to atmospheric conditions, with columns likely measuring these at specific heights or in specific orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by different location and create differend panda frames for it, and for each of them we create a sub_df and target, and we do .shift(1) what it will do it will shift by one, it will give us yesterdays value if we shift by one\n",
    "\n",
    "for that we can create new variable, sub df leg 1 \n",
    "\n",
    "we create a new column, feature engineering, where for each place, what is the observation of the day before, but before this we need to do the train test split, it should also be in a forward looking fashion, take the last three days for each location as test set, in a forecasting problem we dont do long term prediction, its okay to take the alst 3 days as test set. Its gonna be location based. \n",
    "\n",
    "What are the two? create yesterday observation as a feature with groupby magic\n",
    "generic thing create test set, forward looking test, last three days, it needs also group by manipulation\n",
    "\n",
    "\n",
    "MAPE mean absolute error \n",
    "\n",
    "\n",
    "train set has \n",
    "day 1 - day 60\n",
    "\n",
    "and test set\n",
    "\n",
    "day 61, 62, 63\n",
    "\n",
    "we need to add column which location, some are target Place_ID  - NEW column lag 1 --> day before (once created we do XGBOOST or linear regression on it - time aspect), first we create lage 1\n",
    "\n",
    "sub df for each location as part of the est set\n",
    "\n",
    "pearson correlation can be computed even with missing values, to see which features are not so powerful - for feature selection! \n",
    "\n",
    "measure performance, other complexity\n",
    "\n",
    "how do we feed sub dfs in regression?\n",
    "\n",
    "Baseline model lag1 feature - what is the target of the day before - drop the dates where no date before is (like first date) - lag 1 is corresponding to its own place id, so than the date problem is gone and we can feed it in the regression\n",
    "\n",
    "linear regression (Xtrain, Ytrain - polluted yesterday?\n",
    "\n",
    "lag 1 on our test data is the prediction but not the ground root! - to build approximation on approximation is - time series problem extremely noisy - tmrw becomes new data, continuesoly predict on for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Custom Transformer for Group-Wise Imputation\n",
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_col, strategy='mean'):\n",
    "        self.group_col = group_col\n",
    "        self.strategy = strategy\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Separate the group column from the features\n",
    "        self.columns_ = X.columns.drop(self.group_col)\n",
    "        \n",
    "        # Compute group-wise statistics (mean) for each numerical feature\n",
    "        self.impute_values_ = X.groupby(self.group_col).transform(lambda x: x.mean())\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Fill missing values based on group statistics\n",
    "        X_filled = X.copy()\n",
    "        for col in self.columns_:\n",
    "            # Use the group-wise mean for each numerical feature\n",
    "            X_filled[col] = X_filled[col].fillna(self.impute_values_[col])\n",
    "        return X_filled.drop(columns=[self.group_col])\n",
    "\n",
    "# Step 2: Define numerical and categorical features\n",
    "numerical_features = [\"L3_CO_CO_column_number_density\", \n",
    "                      \"L3_HCHO_tropospheric_HCHO_column_number_density\",\n",
    "                      \"L3_NO2_NO2_column_number_density\", \n",
    "                      \"L3_O3_O3_column_number_density\", \n",
    "                      \"u_component_of_wind_10m_above_ground\"]\n",
    "\n",
    "categorical_features = ['Place_ID']  # Ensure 'Place_ID' exists in your DataFrame\n",
    "\n",
    "# Step 3: Preprocess the data (Group-Wise Imputation + Fallback Imputation + Scaling/Encoding)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('group_imputer', GroupImputer(group_col='Place_ID')),  # Custom group-wise imputation\n",
    "            ('fallback_imputer', SimpleImputer(strategy='mean')),  # Fallback imputer for any remaining NaNs\n",
    "            ('scaler', StandardScaler())  # Standardize the numerical features\n",
    "        ]), numerical_features + ['Place_ID']),  # Pass Place_ID for group imputation\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)  # OneHotEncode Place_ID\n",
    "    ])\n",
    "\n",
    "# Define the target variable\n",
    "y_train = df_train_split[\"target\"]\n",
    "X_train = df_train_split[numerical_features + categorical_features]\n",
    "\n",
    "y_test = df_test_split[\"target\"]\n",
    "X_test = df_test_split[numerical_features + categorical_features]\n",
    "\n",
    "# Step 6: Create a pipeline with preprocessing and linear regression\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Step 7: Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Make predictions and evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate the performance\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {mape :.2f}%')\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Missing Data:\n",
    "You have many features with missing data, some of which have a significant portion of missing values. For example:\n",
    "\n",
    "L3_NO2_NO2_column_number_density and other NO2-related features have 2186 missing values.\n",
    "L3_CH4_CH4_column_volume_mixing_ratio_dry_air and other CH4-related features have 24162 missing values.\n",
    "Large portions of missing data can cause issues with model performance, especially if the missing values are not imputed correctly. Although you're using mean imputation, if the missing data is not random, it could introduce bias.\n",
    "\n",
    "Action: Consider a more sophisticated imputation strategy, such as KNN imputation, or drop features with too many missing values. Alternatively, try models that handle missing data natively, such as XGBoost or LightGBM.\n",
    "\n",
    "2. Outliers:\n",
    "You have a significant number of outliers in your numerical features, particularly:\n",
    "\n",
    "u_component_of_wind_10m_above_ground has 1413 outliers.\n",
    "L3_NO2_NO2_slant_column_number_density has 3567 outliers.\n",
    "L3_CLOUD_cloud_optical_depth has 2721 outliers.\n",
    "Outliers can heavily skew models like linear regression, leading to poor performance.\n",
    "\n",
    "Action:\n",
    "\n",
    "You may want to cap (winsorize) or remove outliers from your dataset.\n",
    "Alternatively, use robust models that are less sensitive to outliers (e.g., Huber Regressor, Gradient Boosting with outlier handling).\n",
    "3. Low Variance Features:\n",
    "You've identified several low-variance features that may not provide much information for the model. These include:\n",
    "\n",
    "L3_NO2_NO2_slant_column_number_density\n",
    "L3_CO_CO_column_number_density\n",
    "L3_SO2_SO2_slant_column_number_density\n",
    "Low-variance features add little value and may increase noise in the model.\n",
    "\n",
    "Action: Drop these low-variance features from your dataset.\n",
    "\n",
    "4. Collinearity:\n",
    "Some of your features are likely highly correlated (multicollinearity), especially since the NO2, O3, and CO-related features are derived from similar sources. Multicollinearity can cause problems with linear models, leading to inflated errors.\n",
    "\n",
    "Action: Use a correlation matrix to remove highly correlated features or use regularization techniques like Ridge or Lasso regression to mitigate this issue.\n",
    "\n",
    "5. Scaling Issues:\n",
    "If your numerical features are not properly scaled, this can cause issues with models like linear regression and gradient boosting.\n",
    "\n",
    "Action: Ensure that your numerical features are standardized (mean = 0, std = 1) or normalized (range 0 to 1), depending on the model you're using.\n",
    "\n",
    "6. Data Type Issues:\n",
    "There don't seem to be any apparent data type issues from your check. However, ensure that the categorical features are properly encoded before feeding them into the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Place_ID' and apply imputation within each group\n",
    "def impute_group(group):\n",
    "    # Example imputation: fill NaN with the mean or median for each column within the group\n",
    "    group.fillna(group.mean(), inplace=True)  # Or use group.median() or forward fill (group.ffill())\n",
    "    return group\n",
    "\n",
    "# Apply the imputation function to each group\n",
    "df_imputed = df_sorted.groupby('Place_ID').apply(impute_group)\n",
    "\n",
    "# If necessary, you can re-sort the imputed data again by 'Place_ID' and 'Date'\n",
    "df_imputed = df_imputed.sort_values(by=['Place_ID', 'Date'])\n",
    "\n",
    "# Optional: Check if there are still any NaNs left\n",
    "remaining_nans = df_imputed.isna().sum()\n",
    "\n",
    "# Display the remaining NaNs\n",
    "print(\"Remaining NaNs after imputation:\\n\", remaining_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (1192019511.py, line 129)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 129\u001b[0;36m\u001b[0m\n\u001b[0;31m    Count: 29,014 (5,521 missing).\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "Summary of Features and Imputation Strategies\n",
    "target\n",
    "Count: Complete data (no missing values).\n",
    "Strategy: No imputation needed.\n",
    "precipitable_water_entire_atmosphere\n",
    "Count: Complete data.\n",
    "Strategy: Mean imputation (normally distributed).\n",
    "relative_humidity_2m_above_ground\n",
    "Count: Complete data.\n",
    "Strategy: Mean imputation (typically normal distribution).\n",
    "specific_humidity_2m_above_ground\n",
    "Count: Complete data.\n",
    "Strategy: Mean imputation (normal variation expected).\n",
    "temperature_2m_above_ground\n",
    "Count: Complete data.\n",
    "Strategy: Mean imputation (central tendency is important).\n",
    "u_component_of_wind_10m_above_ground\n",
    "Count: Complete data.\n",
    "Strategy: Mean imputation (normally distributed wind components).\n",
    "v_component_of_wind_10m_above_ground\n",
    "Count: Complete data.\n",
    "Strategy: Mean imputation (same as u-component).\n",
    "L3_NO2_NO2_column_number_density\n",
    "Count: 28,348 (2,187 missing).\n",
    "Strategy: Median imputation (likely skewed due to pollution spikes).\n",
    "L3_NO2_NO2_slant_column_number_density\n",
    "Count: 28,348 (2,187 missing).\n",
    "Strategy: Median imputation (outliers present).\n",
    "L3_NO2_absorbing_aerosol_index\n",
    "Count: 28,348 (2,187 missing).\n",
    "Strategy: Median imputation (sensitive to extreme values).\n",
    "L3_NO2_cloud_fraction\n",
    "Count: 28,348 (2,187 missing).\n",
    "Strategy: Median imputation (typically binary presence).\n",
    "L3_NO2_sensor_altitude\n",
    "Count: 28,348 (2,187 missing).\n",
    "Strategy: Median imputation (altitude values may be clustered).\n",
    "L3_NO2_sensor_azimuth_angle\n",
    "Count: 28,347 (2,188 missing).\n",
    "Strategy: Median imputation (skewed data requires median).\n",
    "L3_NO2_sensor_zenith_angle\n",
    "Count: 28,347 (2,188 missing).\n",
    "Strategy: Median imputation (protects against outliers).\n",
    "L3_NO2_solar_azimuth_angle\n",
    "Count: 28,347 (2,188 missing).\n",
    "Strategy: Median imputation (similar reasoning).\n",
    "L3_NO2_solar_zenith_angle\n",
    "Count: 28,347 (2,188 missing).\n",
    "Strategy: Median imputation (avoids skew).\n",
    "L3_NO2_stratospheric_NO2_column_number_density\n",
    "Count: 28,347 (2,188 missing).\n",
    "Strategy: Median imputation (high variability).\n",
    "L3_NO2_tropopause_pressure\n",
    "Count: 28,347 (2,188 missing).\n",
    "Strategy: Median imputation (pressure can vary widely).\n",
    "L3_NO2_tropospheric_NO2_column_number_density\n",
    "Count: 21,870 (8,665 missing).\n",
    "Strategy: Median imputation (extreme values).\n",
    "L3_O3_O3_column_number_density\n",
    "Count: 30,253 (missing values).\n",
    "Strategy: Median imputation (affected by environmental factors).\n",
    "L3_O3_O3_effective_temperature\n",
    "Count: 30,253 (missing values).\n",
    "Strategy: Median imputation (can be skewed).\n",
    "L3_O3_cloud_fraction\n",
    "Count: 30,253 (missing values).\n",
    "Strategy: Median imputation (central measure is important).\n",
    "L3_O3_sensor_azimuth_angle\n",
    "Count: 30,253 (missing values).\n",
    "Strategy: Median imputation (outliers can skew results).\n",
    "L3_O3_sensor_zenith_angle\n",
    "Count: 30,253 (missing values).\n",
    "Strategy: Median imputation (similar reasoning).\n",
    "L3_O3_solar_azimuth_angle\n",
    "Count: 30,253 (missing values).\n",
    "Strategy: Median imputation (protect against extremes).\n",
    "L3_O3_solar_zenith_angle\n",
    "Count: 30,253 (missing values).\n",
    "Strategy: Median imputation (avoids skew).\n",
    "L3_CO_CO_column_number_density\n",
    "Count: 25,114 (5,421 missing).\n",
    "Strategy: Median imputation (pollution variability).\n",
    "L3_CO_H2O_column_number_density\n",
    "Count: 25,114 (5,421 missing).\n",
    "Strategy: Median imputation (extreme values may exist).\n",
    "L3_CO_cloud_height\n",
    "Count: 25,114 (5,421 missing).\n",
    "Strategy: Median imputation (high variability).\n",
    "L3_CO_sensor_altitude\n",
    "Count: 25,114 (5,421 missing).\n",
    "Strategy: Median imputation (clustered values).\n",
    "L3_CO_sensor_azimuth_angle\n",
    "Count: 25,114 (5,421 missing).\n",
    "Strategy: Median imputation (affected by outliers).\n",
    "L3_CO_sensor_zenith_angle\n",
    "Count: 25,114 (5,421 missing).\n",
    "Strategy: Median imputation (skewed distribution).\n",
    "L3_CO_solar_azimuth_angle\n",
    "Count: 25,114 (5,421 missing).\n",
    "Strategy: Median imputation (similar reasoning).\n",
    "L3_CO_solar_zenith_angle\n",
    "Count: 25,114 (5,421 missing).\n",
    "Strategy: Median imputation (protect against extremes).\n",
    "L3_HCHO_HCHO_slant_column_number_density\n",
    "Count: 23,115 (7,420 missing).\n",
    "Strategy: Median imputation (environmental influences).\n",
    "L3_HCHO_cloud_fraction\n",
    "Count: 23,115 (7,420 missing).\n",
    "Strategy: Median imputation (central measure is important).\n",
    "L3_HCHO_sensor_azimuth_angle\n",
    "Count: 23,115 (7,420 missing).\n",
    "Strategy: Median imputation (outliers can skew results).\n",
    "L3_HCHO_sensor_zenith_angle\n",
    "Count: 23,115 (7,420 missing).\n",
    "Strategy: Median imputation (avoids skew).\n",
    "L3_HCHO_solar_azimuth_angle\n",
    "Count: 23,115 (7,420 missing).\n",
    "Strategy: Median imputation (protect against extremes).\n",
    "L3_HCHO_solar_zenith_angle\n",
    "Count: 23,115 (7,420 missing).\n",
    "Strategy: Median imputation (similar reasoning).\n",
    "L3_HCHO_tropospheric_HCHO_column_number_density\n",
    "Count: 23,115 (7,420 missing).\n",
    "Strategy: Median imputation (high variability).\n",
    "L3_HCHO_tropospheric_HCHO_column_number_density_amf\n",
    "Count: 23,115 (7,420 missing).\n",
    "Strategy: Median imputation (high variability).\n",
    "L3_CLOUD_cloud_base_height\n",
    "Count: 29,014 (5,521 missing).\n",
    "Strategy: Median imputation (skewed data expected).\n",
    "L3_CLOUD_cloud_base_pressure\n",
    "Count: 29,014 (5,521 missing).\n",
    "Strategy: Median imputation (can vary widely).\n",
    "L3_CLOUD_cloud_optical_depth\n",
    "Count: 29,014 (5,521 missing).\n",
    "Strategy: Median imputation (sensitive to extreme values).\n",
    "L3_CLOUD_cloud_top_height\n",
    "Count: 29,014 (5,521 missing).\n",
    "Strategy: Median imputation (similar reasoning).\n",
    "L3_CLOUD_cloud_top_pressure\n",
    "Count: 29,014 (5,521 missing).\n",
    "Strategy: Median imputation (high variability).\n",
    "L3_CLOUD_surface_albedo\n",
    "Count: 29,014 (5,521 missing).\n",
    "Strategy: Median imputation (skewed data expected).\n",
    "L3_AER_AI_absorbing_aerosol_index\n",
    "Count: 30,339 (196 missing).\n",
    "Strategy: Median imputation (outlier protection).\n",
    "L3_AER_AI_sensor_altitude\n",
    "Count: 30,339 (196 missing).\n",
    "Strategy: Median imputation (clustered values).\n",
    "L3_AER_AI_sensor_azimuth_angle\n",
    "Count: 30,339 (196 missing).\n",
    "Strategy: Median imputation (affected by outliers).\n",
    "L3_AER_AI_sensor_zenith_angle\n",
    "Count: 30,339 (196 missing).\n",
    "Strategy: Median imputation (avoids skew).\n",
    "L3_AER_AI_solar_azimuth_angle\n",
    "Count: 30,339 (196 missing).\n",
    "Strategy: Median imputation (protect against extremes).\n",
    "L3_AER_AI_solar_zenith_angle\n",
    "Count: 30,339 (196 missing).\n",
    "Strategy: Median imputation (similar reasoning).\n",
    "L3_SO2_SO2_column_number_density\n",
    "Count: 23,304 (7,231 missing).\n",
    "Strategy: Median imputation (high variability).\n",
    "L3_SO2_SO2_column_number_density_amf\n",
    "Count: 23,304 (7,231 missing).\n",
    "Strategy: Median imputation (high variability).\n",
    "L3_SO2_SO2_slant_column_number_density\n",
    "Count: 23,304 (7,231 missing).\n",
    "Strategy: Median imputation (sensitive to extreme values).\n",
    "L3_SO2_absorbing_aerosol_index\n",
    "Count: 23,233 (7,302 missing).\n",
    "Strategy: Median imputation (outlier protection).\n",
    "L3_SO2_cloud_fraction\n",
    "Count: 23,304 (7,231 missing).\n",
    "Strategy: Median imputation (central measure is important).\n",
    "L3_SO2_sensor_azimuth_angle\n",
    "Count: 23,304 (7,231 missing).\n",
    "Strategy: Median imputation (affected by outliers).\n",
    "L3_SO2_sensor_zenith_angle\n",
    "Count: 23,304 (7,231 missing).\n",
    "Strategy: Median imputation (avoids skew).\n",
    "L3_SO2_solar_azimuth_angle\n",
    "Count: 23,304 (7,231 missing).\n",
    "Strategy: Median imputation (protect against extremes).\n",
    "L3_SO2_solar_zenith_angle\n",
    "Count: 23,304 (7,231 missing).\n",
    "Strategy: Median imputation (similar reasoning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Check for Data Leakage:\n",
    "Make sure that your test data is not being contaminated with information from the training set. For example, ensure that lag features are calculated only from the past data, not future data.\n",
    "\n",
    "Conclusion:\n",
    "Start by testing more complex models like Random Forest, Gradient Boosting, or XGBoost.\n",
    "Improve feature engineering by adding lag features, rolling averages, and interaction terms.\n",
    "Use log transformation to handle skewness in the target.\n",
    "Regularization and ensemble techniques can also help prevent overfitting.\n",
    "Always validate your results using cross-validation or a validation set.\n",
    "By applying these strategies, you should see significant improvement in your RMSE scores!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
